{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Convolutional Neural Network - CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* To classify the image set\n",
        "* To test the performance of the classification using resizing vs resizing + padding\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* CSVs outputted from ETL for resizing and resizing + padding\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import base64\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, RandomFlip, RandomRotation, RandomZoom\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Eddie\\\\Documents\\\\CodeInstitute Workspace\\\\Capstone Project'"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "if os.path.basename(current_dir).lower() == \"jupyter_notebooks\":\n",
        "    os.chdir(os.path.dirname(current_dir))\n",
        "    print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Eddie\\\\Documents\\\\CodeInstitute Workspace\\\\Capstone Project'"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subfolder</th>\n",
              "      <th>luminance</th>\n",
              "      <th>contrast</th>\n",
              "      <th>image_path</th>\n",
              "      <th>grey_image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anger</td>\n",
              "      <td>202.218752</td>\n",
              "      <td>42.869499</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\image_0.png</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\grey_image_0.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>anger</td>\n",
              "      <td>134.196622</td>\n",
              "      <td>101.303135</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\image_1.png</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\grey_image_1.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>109.144209</td>\n",
              "      <td>51.433215</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\image_2.png</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\grey_image_2.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>anger</td>\n",
              "      <td>140.261702</td>\n",
              "      <td>89.338355</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\image_3.png</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\grey_image_3.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>anger</td>\n",
              "      <td>143.852701</td>\n",
              "      <td>81.434234</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\image_4.png</td>\n",
              "      <td>./Data/Processed/Resized+Padded\\grey_image_4.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subfolder   luminance    contrast  \\\n",
              "0     anger  202.218752   42.869499   \n",
              "1     anger  134.196622  101.303135   \n",
              "2     anger  109.144209   51.433215   \n",
              "3     anger  140.261702   89.338355   \n",
              "4     anger  143.852701   81.434234   \n",
              "\n",
              "                                    image_path  \\\n",
              "0  ./Data/Processed/Resized+Padded\\image_0.png   \n",
              "1  ./Data/Processed/Resized+Padded\\image_1.png   \n",
              "2  ./Data/Processed/Resized+Padded\\image_2.png   \n",
              "3  ./Data/Processed/Resized+Padded\\image_3.png   \n",
              "4  ./Data/Processed/Resized+Padded\\image_4.png   \n",
              "\n",
              "                                    grey_image_path  \n",
              "0  ./Data/Processed/Resized+Padded\\grey_image_0.png  \n",
              "1  ./Data/Processed/Resized+Padded\\grey_image_1.png  \n",
              "2  ./Data/Processed/Resized+Padded\\grey_image_2.png  \n",
              "3  ./Data/Processed/Resized+Padded\\grey_image_3.png  \n",
              "4  ./Data/Processed/Resized+Padded\\grey_image_4.png  "
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resize = pd.read_csv(\"./Data/Processed/df_resized.csv\")\n",
        "df_resize_padding = pd.read_csv(\"./Data/Processed/df_resized_padded.csv\")\n",
        "\n",
        "# Display the first few rows of each DataFrame\n",
        "df_resize.head()\n",
        "df_resize_padding.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n#no longer using base64 encoding to store images in dataframes, instead storing file paths\\n\\n#convert back to numpy arrays\\ndef base64_to_cv2_array(b64_string):\\n    # Decode base64 → bytes\\n    img_bytes = base64.b64decode(b64_string)\\n\\n    # Convert bytes → 1D uint8 array (image buffer)\\n    img_array = np.frombuffer(img_bytes, dtype=np.uint8)\\n\\n    return cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\\n\\ndf_resize['image'] = df_resize['image'].apply(lambda x: base64_to_cv2_array(x))\\ndf_resize['grey_image'] = df_resize['grey_image'].apply(lambda x: base64_to_cv2_array(x))\\n\\ndf_resize_padding['image'] = df_resize_padding['image'].apply(lambda x: base64_to_cv2_array(x))\\ndf_resize_padding['grey_image'] = df_resize_padding['grey_image'].apply(lambda x: base64_to_cv2_array(x))\\n\""
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#no longer using base64 encoding to store images in dataframes, instead storing file paths\n",
        "\n",
        "#convert back to numpy arrays\n",
        "def base64_to_cv2_array(b64_string):\n",
        "    # Decode base64 → bytes\n",
        "    img_bytes = base64.b64decode(b64_string)\n",
        "\n",
        "    # Convert bytes → 1D uint8 array (image buffer)\n",
        "    img_array = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "\n",
        "    return cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "df_resize['image'] = df_resize['image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "df_resize['grey_image'] = df_resize['grey_image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "\n",
        "df_resize_padding['image'] = df_resize_padding['image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "df_resize_padding['grey_image'] = df_resize_padding['grey_image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encode labels and split data into training, validation, and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "#encode labels\n",
        "encoder = LabelEncoder()\n",
        "df_resize['encoded_labels'] = encoder.fit_transform(df_resize['subfolder'])\n",
        "df_resize_padding['encoded_labels'] = encoder.fit_transform(df_resize_padding['subfolder'])\n",
        "\n",
        "#split data into training, validation, and testing\n",
        "train_df_resize, temp_df_resize = train_test_split(\n",
        "    df_resize, test_size=0.4, random_state=3, stratify=df_resize['encoded_labels']\n",
        ")\n",
        "val_df_resize, test_df_resize = train_test_split(\n",
        "    temp_df_resize, test_size=0.65, random_state=3, stratify=temp_df_resize['encoded_labels']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "encoded_labels\n",
              "3    60\n",
              "5    58\n",
              "0    56\n",
              "1    52\n",
              "4    44\n",
              "2    42\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df_resize['encoded_labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colour images - resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Eddie\\Documents\\CodeInstitute Workspace\\Capstone Project\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "data_augmentation = Sequential([\n",
        "    RandomFlip(\"horizontal\"),        # randomly flip left/right\n",
        "    RandomRotation(0.1),             # rotate up to ±10%\n",
        "    RandomZoom(0.1),                 # zoom in/out up to ±10%\n",
        "])\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "'''\n",
        "stacked_images = np.stack(train_df_resize['image'].values).astype('float16')/255.0\n",
        "stacked_labels = np.stack(train_df_resize['encoded_labels'].values)\n",
        "stacked_val_images = np.stack(val_df_resize['image'].values).astype('float16')/255.0\n",
        "stacked_val_labels = np.stack(val_df_resize['encoded_labels'].values)\n",
        "stacked_test_images = np.stack(test_df_resize['image'].values).astype('float16')/255.0\n",
        "stacked_test_labels = np.stack(test_df_resize['encoded_labels'].values)\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def load_and_preprocess(path, label, augment=False):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_png(img, channels=3)          # dtype uint8 [0..255]\n",
        "    img = tf.cast(img, tf.float16) / 255.0              # normalize + convert per batch\n",
        "    if augment:\n",
        "        img = data_augmentation(img)\n",
        "    return img, label\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_df_resize[\"image_path\"], train_df_resize[\"encoded_labels\"]))\n",
        "train_ds = train_ds.map(lambda x, y: load_and_preprocess(x, y, augment=True), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_df_resize[\"image_path\"], val_df_resize[\"encoded_labels\"]))\n",
        "val_ds = val_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_df_resize[\"image_path\"], test_df_resize[\"encoded_labels\"]))\n",
        "test_ds = test_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#model.fit(stacked_images, stacked_labels, epochs=5, validation_data=(stacked_val_images, stacked_val_labels))\n",
        "\n",
        "#test_loss, test_accuracy = model.evaluate(stacked_test_images, stacked_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train batch <dtype: 'float32'> 0.0 1.0\n"
          ]
        }
      ],
      "source": [
        "def inspect_arr(a, name):\n",
        "    import numpy as np\n",
        "    a = np.asarray(a)\n",
        "    print(name, \"dtype\", a.dtype, \"min\", a.min(), \"max\", a.max(), \"shape\", a.shape)\n",
        "\n",
        "# If using tf.data: fetch one batch and inspect\n",
        "for x,y in train_ds.take(1):\n",
        "    print(\"train batch\", x.dtype, x.numpy().min(), x.numpy().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 683ms/step - accuracy: 0.1944 - loss: 2.1368 - val_accuracy: 0.2143 - val_loss: 1.7589\n",
            "Epoch 2/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 648ms/step - accuracy: 0.2472 - loss: 1.7371 - val_accuracy: 0.2500 - val_loss: 1.7200\n",
            "Epoch 3/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 755ms/step - accuracy: 0.3083 - loss: 1.7015 - val_accuracy: 0.2857 - val_loss: 1.7230\n",
            "Epoch 4/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 640ms/step - accuracy: 0.3056 - loss: 1.7100 - val_accuracy: 0.2679 - val_loss: 1.7381\n",
            "Epoch 5/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 635ms/step - accuracy: 0.3389 - loss: 1.6503 - val_accuracy: 0.3452 - val_loss: 1.6773\n",
            "Epoch 6/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 624ms/step - accuracy: 0.3361 - loss: 1.6380 - val_accuracy: 0.3095 - val_loss: 1.6823\n",
            "Epoch 7/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 628ms/step - accuracy: 0.3667 - loss: 1.6031 - val_accuracy: 0.3095 - val_loss: 1.6927\n",
            "Epoch 8/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 633ms/step - accuracy: 0.4028 - loss: 1.5410 - val_accuracy: 0.3333 - val_loss: 1.6642\n",
            "Epoch 9/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 633ms/step - accuracy: 0.4028 - loss: 1.5530 - val_accuracy: 0.3690 - val_loss: 1.6005\n",
            "Epoch 10/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 618ms/step - accuracy: 0.4444 - loss: 1.4775 - val_accuracy: 0.3571 - val_loss: 1.6344\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 0.3269 - loss: 1.5905\n"
          ]
        }
      ],
      "source": [
        "model.fit(train_ds, epochs=10, validation_data=(val_ds))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.12 (capstone-venv)",
      "language": "python",
      "name": "capstone-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
