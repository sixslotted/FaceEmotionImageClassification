{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Convolutional Neural Network - CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* To classify the image set\n",
        "* To test the performance of the classification using resizing vs resizing + padding\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* CSVs outputted from ETL for resizing and resizing + padding\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import base64\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Eddie\\\\Documents\\\\CodeInstitute Workspace\\\\Capstone Project\\\\jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "if os.path.basename(current_dir).lower() == \"jupyter_notebooks\":\n",
        "    os.chdir(os.path.dirname(current_dir))\n",
        "    print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Eddie\\\\Documents\\\\CodeInstitute Workspace\\\\Capstone Project'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subfolder</th>\n",
              "      <th>image</th>\n",
              "      <th>grey_image</th>\n",
              "      <th>luminance</th>\n",
              "      <th>contrast</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anger</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...</td>\n",
              "      <td>202.218752</td>\n",
              "      <td>42.869499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>anger</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...</td>\n",
              "      <td>134.196622</td>\n",
              "      <td>101.303135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...</td>\n",
              "      <td>109.144209</td>\n",
              "      <td>51.433215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>anger</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...</td>\n",
              "      <td>140.261702</td>\n",
              "      <td>89.338355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>anger</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...</td>\n",
              "      <td>iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...</td>\n",
              "      <td>143.852701</td>\n",
              "      <td>81.434234</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subfolder                                              image  \\\n",
              "0     anger  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...   \n",
              "1     anger  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...   \n",
              "2     anger  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...   \n",
              "3     anger  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...   \n",
              "4     anger  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAIAAAAxBA+LAA...   \n",
              "\n",
              "                                          grey_image   luminance    contrast  \n",
              "0  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...  202.218752   42.869499  \n",
              "1  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...  134.196622  101.303135  \n",
              "2  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...  109.144209   51.433215  \n",
              "3  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...  140.261702   89.338355  \n",
              "4  iVBORw0KGgoAAAANSUhEUgAAAlgAAAJYCAAAAACbDccAAA...  143.852701   81.434234  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resize = pd.read_csv(\"./Data/Processed/df_resized.csv\")\n",
        "df_resize_padding = pd.read_csv(\"./Data/Processed/df_resized_padded.csv\")\n",
        "\n",
        "# Display the first few rows of each DataFrame\n",
        "df_resize.head()\n",
        "df_resize_padding.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#convert back to numpy arrays\n",
        "def base64_to_cv2_array(b64_string):\n",
        "    # Decode base64 → bytes\n",
        "    img_bytes = base64.b64decode(b64_string)\n",
        "\n",
        "    # Convert bytes → 1D uint8 array (image buffer)\n",
        "    img_array = np.frombuffer(img_bytes, dtype=np.uint8)\n",
        "\n",
        "    return cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "df_resize['image'] = df_resize['image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "df_resize['grey_image'] = df_resize['grey_image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "\n",
        "df_resize_padding['image'] = df_resize_padding['image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "df_resize_padding['grey_image'] = df_resize_padding['grey_image'].apply(lambda x: base64_to_cv2_array(x))\n",
        "\n",
        "#normalize pixel values\n",
        "df_resize['image'] = df_resize['image'].apply(lambda x: x.astype('float32') / 255.0)\n",
        "df_resize['grey_image'] = df_resize['grey_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subfolder</th>\n",
              "      <th>image</th>\n",
              "      <th>grey_image</th>\n",
              "      <th>luminance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>anger</td>\n",
              "      <td>[[[204, 203, 205], [197, 196, 198], [195, 194,...</td>\n",
              "      <td>[[204, 197, 195, 197, 197, 197, 198, 198, 198,...</td>\n",
              "      <td>202.218752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>anger</td>\n",
              "      <td>[[[251, 251, 251], [251, 251, 251], [252, 252,...</td>\n",
              "      <td>[[251, 251, 252, 252, 252, 252, 252, 252, 253,...</td>\n",
              "      <td>134.196622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>[[[8, 44, 20], [9, 45, 21], [13, 53, 26], [15,...</td>\n",
              "      <td>[[33, 34, 40, 44, 47, 52, 55, 57, 59, 61, 63, ...</td>\n",
              "      <td>109.144209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>anger</td>\n",
              "      <td>[[[209, 199, 182], [209, 199, 182], [210, 200,...</td>\n",
              "      <td>[[195, 195, 196, 196, 196, 197, 197, 197, 198,...</td>\n",
              "      <td>140.261702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>anger</td>\n",
              "      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n",
              "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
              "      <td>143.852701</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subfolder                                              image  \\\n",
              "0     anger  [[[204, 203, 205], [197, 196, 198], [195, 194,...   \n",
              "1     anger  [[[251, 251, 251], [251, 251, 251], [252, 252,...   \n",
              "2     anger  [[[8, 44, 20], [9, 45, 21], [13, 53, 26], [15,...   \n",
              "3     anger  [[[209, 199, 182], [209, 199, 182], [210, 200,...   \n",
              "4     anger  [[[255, 255, 255], [255, 255, 255], [255, 255,...   \n",
              "\n",
              "                                          grey_image   luminance  \n",
              "0  [[204, 197, 195, 197, 197, 197, 198, 198, 198,...  202.218752  \n",
              "1  [[251, 251, 252, 252, 252, 252, 252, 252, 253,...  134.196622  \n",
              "2  [[33, 34, 40, 44, 47, 52, 55, 57, 59, 61, 63, ...  109.144209  \n",
              "3  [[195, 195, 196, 196, 196, 197, 197, 197, 198,...  140.261702  \n",
              "4  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  143.852701  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resize[].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encode labels and split data into training, validation, and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#encode labels\n",
        "encoder = LabelEncoder()\n",
        "df_resize['encoded_labels'] = encoder.fit_transform(df_resize['subfolder'])\n",
        "df_resize_padding['encoded_labels'] = encoder.fit_transform(df_resize_padding['subfolder'])\n",
        "\n",
        "#split data into training, validation, and testing\n",
        "train_df_resize, temp_df_resize = train_test_split(\n",
        "    df_resize, test_size=0.4, random_state=1, stratify=df_resize['encoded_labels']\n",
        ")\n",
        "val_df_resize, test_df_resize = train_test_split(\n",
        "    temp_df_resize, test_size=0.75, random_state=1, stratify=temp_df_resize['encoded_labels']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "encoded_labels\n",
              "3    230\n",
              "5    224\n",
              "0    214\n",
              "1    201\n",
              "4    168\n",
              "2    163\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_resize['encoded_labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colour images - resized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 1.45 GiB for an array with shape (720, 600, 600, 3) and data type float16",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m model = Sequential([\n\u001b[32m      2\u001b[39m     Conv2D(\u001b[32m32\u001b[39m, (\u001b[32m3\u001b[39m,\u001b[32m3\u001b[39m), activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m, input_shape=(\u001b[32m600\u001b[39m, \u001b[32m600\u001b[39m, \u001b[32m3\u001b[39m)),\n\u001b[32m      3\u001b[39m     MaxPooling2D((\u001b[32m2\u001b[39m,\u001b[32m2\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     Dense(\u001b[32m6\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m ])\n\u001b[32m     11\u001b[39m model.compile(optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m               loss=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m               metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m stacked_images = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df_resize\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfloat16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m/\u001b[32m255.0\u001b[39m\n\u001b[32m     16\u001b[39m stacked_labels = np.stack(train_df_resize[\u001b[33m'\u001b[39m\u001b[33mencoded_labels\u001b[39m\u001b[33m'\u001b[39m].values)\n\u001b[32m     17\u001b[39m stacked_val_images = np.stack(val_df_resize[\u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m].values).astype(\u001b[33m'\u001b[39m\u001b[33mfloat16\u001b[39m\u001b[33m'\u001b[39m)/\u001b[32m255.0\u001b[39m\n",
            "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.45 GiB for an array with shape (720, 600, 600, 3) and data type float16"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(600, 600, 3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "stacked_images = np.stack(train_df_resize['image'].values).astype('float16')/255.0\n",
        "stacked_labels = np.stack(train_df_resize['encoded_labels'].values)\n",
        "stacked_val_images = np.stack(val_df_resize['image'].values).astype('float16')/255.0\n",
        "stacked_val_labels = np.stack(val_df_resize['encoded_labels'].values)\n",
        "stacked_test_images = np.stack(test_df_resize['image'].values).astype('float16')/255.0\n",
        "stacked_test_labels = np.stack(test_df_resize['encoded_labels'].values)\n",
        "\n",
        "#model.fit(stacked_images, stacked_labels, epochs=5, validation_data=(stacked_val_images, stacked_val_labels))\n",
        "\n",
        "#test_loss, test_accuracy = model.evaluate(stacked_test_images, stacked_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m969s\u001b[0m 40s/step - accuracy: 0.2014 - loss: 3876.8958 - val_accuracy: 0.2667 - val_loss: 5.5711\n",
            "Epoch 2/5\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 30s/step - accuracy: 0.5764 - loss: 1.3462 - val_accuracy: 0.2917 - val_loss: 2.0098\n",
            "Epoch 3/5\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 30s/step - accuracy: 0.8694 - loss: 0.5030 - val_accuracy: 0.3417 - val_loss: 2.4563\n",
            "Epoch 4/5\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27603s\u001b[0m 1254s/step - accuracy: 0.9611 - loss: 0.2494 - val_accuracy: 0.3417 - val_loss: 3.3746\n",
            "Epoch 5/5\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 34s/step - accuracy: 0.9833 - loss: 0.0969 - val_accuracy: 0.3333 - val_loss: 3.8208\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 724ms/step - accuracy: 0.2722 - loss: 4.6004\n"
          ]
        }
      ],
      "source": [
        "model.fit(stacked_images, stacked_labels, epochs=5, validation_data=(stacked_val_images, stacked_val_labels))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(stacked_test_images, stacked_test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.12 (capstone-venv)",
      "language": "python",
      "name": "capstone-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
